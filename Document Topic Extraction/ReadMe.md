# Document Topic Extraction

## Document Topic Extraction

This project uses an open source dataset from Sklearn: from sklearn.datasets import fetch_20newsgroups. The data set consists of new articles that are already classified into twenty different topics. 

### Latent Dirichlet Allocation

Latent Dirichlet Allocation (LDA) looks at all corpus of words in a document, and clusters the documents together through probability that words occur together in a document and then together in other documents. LDA allows you to set the number of clusters yourself.

Latent Dirichlet Allocation (LDA) is a text analysis tool that gathers documents into groups or topics by the frequency and similarity of terms within each document.

Example: https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24

### pyLDAvis
Look at this cool package that enables interactive

Saliency: http://vis.stanford.edu/files/2012-Termite-AVI.pdf We define term saliency as follows. For a given word w, we compute its conditional probability P(T|w): the likelihood that observed word w was generated by latent topic T. We also compute the marginal probability P(T): the likelihood that any randomly-selected word w 0 was generated by topic T. We define the distinctiveness of word w as the Kullback-Leibler divergence between P(T|w) and P(T). This formulation describes (in an information-theoretic sense) how informative the specific term w is for determining the generating topic, versus a randomly-selected term w0.

*The saliency of a term is defined by the product: saliency(w) = P(w) Ã— distinctiveness(w)

Relevance: https://nlp.stanford.edu/events/illvi2014/papers/sievert-illvi2014.pdf

We also propose a novel measure, relevance, by which to rank terms within topics to aid in the task of topic interpretation, and we present results from a user study that show that ranking terms in decreasing order of probability is suboptimal for topic interpretation.

See the formula of calculating probability of the word in relation to the topic in the paper.

### HDP Model
Hierarchical Dirichlet Process Model - determines the number of topics within the model. Very similar to LDA but instead of user input for the number of topics, the model determines the number of topics.

### Coherence
https://datascienceplus.com/evaluation-of-topic-modeling-topic-coherence/ http://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf

U-mass coherence measures to compare a word only to the preceding and succeeding words respectively, or measures the intrinsic coherence.

c_v coherence, or UCI measure, every single word is paired with every other single word. This is also called extrinsic coherence.

In the Topic Evaluation paper they find that UCI measure performs better than u-mass overall. This paper brings up some additional coherence measures that should be looked into as the project progresses.

Measuring coherence helps us give a quantitative measurement to the unsupervised learning used in LDA topic modeling.

## Semi-Supervised LDA Document Topic Classification

Semi-Supervised Topic Extraction with Corex Topic
https://medium.com/pew-research-center-decoded/overcoming-the-limitations-of-topic-models-with-a-semi-supervised-approach-b947374e0455 https://gist.github.com/patrickvankessel/0d5bd690910edece831dbdf32fb2fb2d https://github.com/gregversteeg/corex_topic

Resources:
- https://github.com/vi3k6i5/GuidedLDA/blob/master/examples/example_seeded_lda.py
- https://www.freecodecamp.org/news/how-we-changed-unsupervised-lda-to-semi-supervised-guidedlda-e36a95f3a164/

Resources:
- https://github.com/priya-dwivedi/Deep-Learning/blob/master/topic_modeling/LDA_Newsgroup.ipynb
- https://towardsdatascience.com/applying-machine-learning-to-classify-an-unsupervised-text-document-e7bb6265f52
- https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/
